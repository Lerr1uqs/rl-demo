"""
è¿™æ˜¯ä¸€ä¸ªRLæ¡†æ¶çš„ä¸»ç¨‹åºå…¥å£æ–‡ä»¶
ä¸»è¦åŠŸèƒ½ï¼šæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨RLæ¡†æ¶è®­ç»ƒä¸åŒçš„ç®—æ³•
"""
from rich import print
from rich.traceback import Traceback
from rich.console import Console
from loguru import logger
from typing import List

from rlf import (
    MazeEnv,
    DQNAgent,
    PGAgent,
    PPOAgent,
    MazeTrainer,
    DQNConfig,
    PPOConfig
)
from rlf.agents.base import BaseAgent
from rlf.schemas import TrainingConfig


def main() -> None:
    """ä¸»å‡½æ•°"""
    # å®šä¹‰è¿·å®«åœ°å›¾
    maze_map: List[str] = [
        "RWWWWWWWW",
        "RRRRTRRRR",
        "WRWRWWRWG",
        "WRRRBRRRR",
        "WWWWWWWWW",
    ]

    # åˆ›å»ºç¯å¢ƒ
    env = MazeEnv(maze_map)
    print(f"ğŸŒ ç¯å¢ƒåˆ›å»ºæˆåŠŸ: {env.height}x{env.width} è¿·å®«")
    print(f"   çŠ¶æ€ç©ºé—´: {env.state_space}")
    print(f"   åŠ¨ä½œç©ºé—´: {env.action_space}\n")

    # é€‰æ‹©è¦è®­ç»ƒçš„ç®—æ³•
    print("ğŸ¯ é€‰æ‹©è¦è®­ç»ƒçš„ç®—æ³•:")
    print("   1. DQN (Off-Policy)")
    print("   2. Policy Gradient (On-Policy)")
    print("   3. PPO (On-Policy with Limited Reuse)")

    # é»˜è®¤è®­ç»ƒDQN
    choice = 1

    agent: BaseAgent
    if choice == 1:
        # åˆ›å»ºDQN Agent
        dqn_config = DQNConfig(
            learning_rate=0.001,
            gamma=0.99,
            hidden_dim=128,
            buffer_size=50000,
            batch_size=64,
            epsilon=1.0,
            epsilon_decay=0.995,
            epsilon_min=0.01,
            update_target_freq=100
        )
        agent = DQNAgent(
            state_dim=env.state_space,
            action_dim=env.action_space,
            config=dqn_config
        )
    elif choice == 2:
        # åˆ›å»ºPolicy Gradient Agent
        pg_config = TrainingConfig(
            learning_rate=0.001,
            gamma=0.99,
            hidden_dim=128
        )
        agent = PGAgent(
            state_dim=env.state_space,
            action_dim=env.action_space,
            config=pg_config
        )
    elif choice == 3:
        # åˆ›å»ºPPO Agent
        ppo_config = PPOConfig(
            learning_rate=0.001,
            gamma=0.99,
            hidden_dim=128,
            gae_lambda=0.95,
            clip_epsilon=0.2,
            ppo_epochs=4
        )
        agent = PPOAgent(
            state_dim=env.state_space,
            action_dim=env.action_space,
            config=ppo_config
        )
    else:
        raise ValueError("æ— æ•ˆçš„é€‰æ‹©")

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = MazeTrainer(env, agent, save_data=True)

    # è®­ç»ƒ
    result = trainer.train(
        num_episodes=1000,
        print_freq=50,
        render_freq=100
    )

    print("\n" + "="*60)
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print("="*60)
    print(f"ğŸ“Š è®­ç»ƒç»“æœ:")
    print(f"   æ€»æ—¶é—´: {result.total_time:.2f}s")
    print(f"   æœ€ä½³å¥–åŠ±: {result.best_reward:.2f}")
    print(f"   æœ€å50è½®å¹³å‡å¥–åŠ±: {result.final_avg_reward:.2f}")
    print("="*60)

    trainer.plot_training_curves()

    # æœ€ç»ˆæ¼”ç¤º
    print("\nğŸ® æœ€ç»ˆæ¼”ç¤º:")
    # trainer.demo(render=True)


if __name__ == "__main__":
    console = Console()

    try:
        main()
    except Exception as e:
        t = Traceback.from_exception(type(e), e, e.__traceback__)
        with console.capture() as capture:
            console.print(t)
        rich_output = capture.get()
        logger.info("\n" + rich_output)
