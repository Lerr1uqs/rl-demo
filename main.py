"""
è¿™æ˜¯ä¸€ä¸ªRLæ¡†æ¶çš„ä¸»ç¨‹åºå…¥å£æ–‡ä»¶
ä¸»è¦åŠŸèƒ½ï¼šæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨RLæ¡†æ¶è®­ç»ƒä¸åŒçš„ç®—æ³•
"""
from rich import print
from rich.traceback import Traceback
from rich.console import Console
from loguru import logger
from typing import List
from enum import StrEnum
import typer

from rlf import (
    MazeEnv,
    DQNAgent,
    PGAgent,
    PPOAgent,
    MazeTrainer,
    DQNConfig,
    PPOConfig
)
from rlf.agents.base import BaseAgent
from rlf.schemas import TrainingConfig


app = typer.Typer(add_completion=False)


class SupportedAlgorithm(StrEnum):
    """æ”¯æŒçš„ç®—æ³•ç±»å‹"""
    DQN = "dqn"
    PG = "pg"
    PPO = "ppo"


@app.command()
def main(
    algorithm: SupportedAlgorithm = typer.Option(
        SupportedAlgorithm.DQN,
        "--algorithm",
        "-a",
        help="é€‰æ‹©è¦è®­ç»ƒçš„ç®—æ³•: dqn/pg/ppo"
    ),
    export_dir: str = typer.Option(
        "./training_data",
        "--export-dir",
        "-o",
        help="è®­ç»ƒæ•°æ®å¯¼å‡ºç›®å½•"
    )
) -> None:
    """ä¸»å‡½æ•°"""
    assert export_dir.strip()

    # å®šä¹‰è¿·å®«åœ°å›¾
    maze_map: List[str] = [
        "RWWWWWWWW",
        "RRRRTRRRR",
        "WRWRWWRWG",
        "WRRRBRRRR",
        "WWWWWWWWW",
    ]

    # åˆ›å»ºç¯å¢ƒ
    env = MazeEnv(maze_map)
    print(f"ğŸŒ ç¯å¢ƒåˆ›å»ºæˆåŠŸ: {env.height}x{env.width} è¿·å®«")
    print(f"   çŠ¶æ€ç©ºé—´: {env.state_space}")
    print(f"   åŠ¨ä½œç©ºé—´: {env.action_space}\n")

    # é€‰æ‹©è¦è®­ç»ƒçš„ç®—æ³•
    print("ğŸ¯ é€‰æ‹©è¦è®­ç»ƒçš„ç®—æ³•:")
    print("   dqn: DQN (Off-Policy)")
    print("   pg: Policy Gradient (On-Policy)")
    print("   ppo: PPO (On-Policy with Limited Reuse)")
    print(f"   å½“å‰é€‰æ‹©: {algorithm.value}")

    agent: BaseAgent
    if algorithm == SupportedAlgorithm.DQN:
        # åˆ›å»ºDQN Agent
        dqn_config = DQNConfig(
            learning_rate=0.001,
            gamma=0.99,
            hidden_dim=128,
            buffer_size=50000,
            batch_size=64,
            epsilon=1.0,
            epsilon_decay=0.995,
            epsilon_min=0.01,
            update_target_freq=100
        )
        agent = DQNAgent(
            state_dim=env.state_space,
            action_dim=env.action_space,
            config=dqn_config
        )
    elif algorithm == SupportedAlgorithm.PG:
        # åˆ›å»ºPolicy Gradient Agent
        pg_config = TrainingConfig(
            learning_rate=0.001,
            gamma=0.99,
            hidden_dim=128
        )
        agent = PGAgent(
            state_dim=env.state_space,
            action_dim=env.action_space,
            config=pg_config
        )
    elif algorithm == SupportedAlgorithm.PPO:
        # åˆ›å»ºPPO Agent
        ppo_config = PPOConfig(
            learning_rate=0.001,
            gamma=0.99,
            hidden_dim=128,
            gae_lambda=0.95,
            clip_epsilon=0.2,
            ppo_epochs=4
        )
        agent = PPOAgent(
            state_dim=env.state_space,
            action_dim=env.action_space,
            config=ppo_config
        )
    else:
        raise ValueError("æ— æ•ˆçš„é€‰æ‹©")

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = MazeTrainer(
        env,
        agent,
        save_data=True,
        save_dir=export_dir
    )

    # è®­ç»ƒ
    result = trainer.train(
        num_episodes=500,
        print_freq=50,
        render_freq=100
    )

    print("\n" + "="*60)
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print("="*60)
    print(f"ğŸ“Š è®­ç»ƒç»“æœ:")
    print(f"   æ€»æ—¶é—´: {result.total_time:.2f}s")
    print(f"   æœ€ä½³å¥–åŠ±: {result.best_reward:.2f}")
    print(f"   æœ€å50è½®å¹³å‡å¥–åŠ±: {result.final_avg_reward:.2f}")
    print("="*60)

    trainer.plot_training_curves()

    # æœ€ç»ˆæ¼”ç¤º
    print("\nğŸ® æœ€ç»ˆæ¼”ç¤º:")
    # trainer.demo(render=True)


if __name__ == "__main__":
    console = Console()

    try:
        app()
    except Exception as e:
        t = Traceback.from_exception(type(e), e, e.__traceback__)
        with console.capture() as capture:
            console.print(t)
        rich_output = capture.get()
        logger.info("\n" + rich_output)
