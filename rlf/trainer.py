"""
è¿™æ˜¯ä¸€ä¸ªè®­ç»ƒå™¨æ¨¡å—ï¼Œä¸»è¦åŠŸèƒ½å¦‚ä¸‹ï¼š
å®žçŽ°å¯æ’æ‹”çš„è®­ç»ƒæ¡†æž¶ï¼Œæ”¯æŒå¤šç§Agentçš„è®­ç»ƒå’Œæ¼”ç¤º
"""
import time
import numpy as np
from typing import Optional

from rlf.env.base import MazeEnv
from rlf.agents.base import BaseAgent
from rlf.schemas import TrainingResult, AgentStats, StepResult
from rlf.data_export import TrainingDataSaver


class MazeTrainer:
    """å¯æ’æ‹”çš„è®­ç»ƒæ¡†æž¶"""

    def __init__(self, env: MazeEnv, agent: BaseAgent, save_data: bool = True) -> None:
        self.env: MazeEnv = env
        self.agent: BaseAgent = agent
        self.episode_rewards: list[float] = []
        self.episode_steps: list[int] = []
        self.save_data = save_data
        self.data_saver = None
        if save_data:
            self.data_saver = TrainingDataSaver()

    def train(
        self,
        num_episodes: int = 500,
        print_freq: int = 50,
        render_freq: int = 100
    ) -> TrainingResult:
        print(f"\n{'='*60}")
        print(f"ðŸš€ å¼€å§‹è®­ç»ƒ: {self.agent.stats.agent_type}")
        print(f"{'='*60}\n")

        start_time: float = time.time()

        for episode in range(num_episodes):
            state: int = self.env.reset()
            episode_reward: float = 0.0
            episode_steps: int = 0
            done: bool = False
            if self.data_saver:
                self.data_saver.record_initial_state(
                    episode=episode,
                    state=state,
                    maze_state=[row[:] for row in self.env.maze_map],
                    agent_pos=list(self.env.agent_pos)
                )

            # æ”¶é›†ä¸€ä¸ªepisodeçš„æ•°æ®
            while not done:
                action: int = self.agent.select_action(state, training=True)
                step_result: StepResult = self.env.step(action)

                # ä¿å­˜æ­¥éª¤æ•°æ®
                if self.data_saver:
                    self.data_saver.record_step(
                        episode=episode,
                        step=episode_steps + 1,
                        state=state,
                        action=action,
                        reward=step_result.reward,
                        maze_state=[row[:] for row in self.env.maze_map],  # æ·±æ‹·è´
                        agent_pos=list(self.env.agent_pos),
                        info=step_result.info,
                        cumulative_reward=episode_reward + step_result.reward
                    )

                self.agent.store_transition(
                    state,
                    action,
                    step_result.reward,
                    step_result.state,
                    step_result.done
                )

                state = step_result.state
                episode_reward += step_result.reward
                episode_steps += 1
                done = step_result.done

            # è®­ç»ƒ
            loss: Optional[float] = self.agent.train()

            agent_stats: AgentStats = self.agent.stats

            # å®Œæˆepisodeè®°å½•
            if self.data_saver:
                self.data_saver.finalize_episode(
                    episode=episode,
                    total_reward=episode_reward,
                    total_steps=episode_steps,
                    success=episode_reward > 50,
                    loss=loss,
                    agent_stats=agent_stats
                )

            self.episode_rewards.append(episode_reward)
            self.episode_steps.append(episode_steps)

            # æ‰“å°è®­ç»ƒä¿¡æ¯
            if (episode + 1) % print_freq == 0:
                avg_reward: float = float(
                    np.mean(self.episode_rewards[-print_freq:])
                )
                avg_steps: float = float(
                    np.mean(self.episode_steps[-print_freq:])
                )
                elapsed_time: float = time.time() - start_time

                print(f"\n{'â”€'*60}")
                print(f"ðŸ“Š Episode {episode + 1}/{num_episodes}")
                print(f"{'â”€'*60}")
                print(f"â±ï¸  Time: {elapsed_time:.2f}s")
                print(f"ðŸŽ¯ Avg Reward (last {print_freq}): {avg_reward:.2f}")
                print(f"ðŸ‘£ Avg Steps: {avg_steps:.2f}")
                print(f"ðŸ“‰ Loss: {loss:.4f}" if loss else "ðŸ“‰ Loss: warming up...")

                print(f"\nðŸ“ˆ Agent Stats:")
                self._print_stats(agent_stats)
                print(f"{'â”€'*60}")

            # æ¸²æŸ“
            if (episode + 1) % render_freq == 0:
                # print(f"\nðŸŽ® Episode {episode + 1} æ¼”ç¤º:")
                # self.demo(render=True)
                pass

        # ä¿å­˜æ•°æ®
        if self.data_saver:
            agent_name = self.agent.stats.agent_type.replace(' ', '_')
            self.data_saver.save(agent_name)

        total_time: float = time.time() - start_time
        best_reward: float = float(max(self.episode_rewards))
        final_avg: float = float(np.mean(self.episode_rewards[-50:]))

        print(f"\n{'='*60}")
        print(f"âœ… è®­ç»ƒå®Œæˆ!")
        print(f"â±ï¸  Total Time: {total_time:.2f}s")
        print(f"ðŸ† Best Reward: {best_reward:.2f}")
        print(f"ðŸ“Š Final Avg Reward (last 50): {final_avg:.2f}")
        print(f"{'='*60}\n")

        return TrainingResult(
            episode_rewards=self.episode_rewards,
            episode_steps=self.episode_steps,
            total_time=total_time,
            best_reward=best_reward,
            final_avg_reward=final_avg
        )

    def _print_stats(self, stats: AgentStats) -> None:
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        if stats.buffer_size > 0:
            print(f"   buffer_size: {stats.buffer_size}")
        if stats.epsilon > 0:
            print(f"   epsilon: {stats.epsilon:.3f}")
        if stats.avg_loss > 0:
            print(f"   avg_loss: {stats.avg_loss:.4f}")
        if stats.train_steps > 0:
            print(f"   train_steps: {stats.train_steps}")
        if stats.current_buffer > 0:
            print(f"   current_buffer: {stats.current_buffer}")
        if stats.ppo_epochs > 0:
            print(f"   ppo_epochs: {stats.ppo_epochs}")
        if stats.avg_policy_loss > 0:
            print(f"   avg_policy_loss: {stats.avg_policy_loss:.4f}")
        if stats.avg_value_loss > 0:
            print(f"   avg_value_loss: {stats.avg_value_loss:.4f}")
        if stats.total_reuses > 0:
            print(f"   total_reuses: {stats.total_reuses}")
        if stats.data_usage:
            print(f"   data_usage: {stats.data_usage}")

    def demo(self, render: bool = True) -> float:
        """æ¼”ç¤ºè®­ç»ƒå¥½çš„agent"""
        state: int = self.env.reset()
        done: bool = False
        total_reward: float = 0.0
        steps: int = 0

        if render:
            self.env.render()

        while not done and steps < 50:
            action: int = self.agent.select_action(state, training=False)
            step_result: StepResult = self.env.step(action)
            state = step_result.state
            total_reward += step_result.reward
            steps += 1
            done = step_result.done

            if render:
                time.sleep(0.1)
                self.env.render()

        return total_reward
