# import gym
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from collections import deque
import random
from typing import List, Tuple, Optional, Deque, Any
from dataclasses import dataclass, field
from pydantic import BaseModel, Field
import time
from rich import print
from rlf.schemas import ACTION_DELTAS, ACTION_ORDER, ACTION_SYMBOLS

# ==================== æ•°æ®ç±»å®šä¹‰ ====================

@dataclass
class StepResult:
    """ç¯å¢ƒæ­¥è¿›è¿”å›ç»“æœ"""
    state: int
    reward: float
    done: bool
    info: 'StepInfo'


@dataclass
class StepInfo:
    """æ­¥è¿›ä¿¡æ¯"""
    hit: str = ""
    timeout: bool = False


@dataclass
class Transition:
    """è½¬æ¢æ•°æ®"""
    state: int
    action: int
    reward: float
    next_state: int
    done: bool


@dataclass
class PPOTransition:
    """PPOè½¬æ¢æ•°æ®ï¼ˆåŒ…å«æ—§ç­–ç•¥çš„log_probå’Œvalueï¼‰"""
    state: int
    action: int
    reward: float
    old_log_prob: float
    value: float


class TrainingConfig(BaseModel):
    """è®­ç»ƒé…ç½®"""
    learning_rate: float = Field(default=0.001, description="å­¦ä¹ ç‡")
    gamma: float = Field(default=0.99, description="æŠ˜æ‰£å› å­")
    hidden_dim: int = Field(default=128, description="éšè—å±‚ç»´åº¦")
    
    class Config:
        arbitrary_types_allowed = True


class DQNConfig(TrainingConfig):
    """DQNé…ç½®"""
    buffer_size: int = Field(default=10000, description="ç»éªŒå›æ”¾ç¼“å†²åŒºå¤§å°")
    batch_size: int = Field(default=64, description="æ‰¹é‡å¤§å°")
    epsilon: float = Field(default=1.0, description="åˆå§‹æ¢ç´¢ç‡")
    epsilon_decay: float = Field(default=0.995, description="æ¢ç´¢ç‡è¡°å‡")
    epsilon_min: float = Field(default=0.01, description="æœ€å°æ¢ç´¢ç‡")
    update_target_freq: int = Field(default=100, description="ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡")


class PPOConfig(TrainingConfig):
    """PPOé…ç½®"""
    gae_lambda: float = Field(default=0.95, description="GAE lambda")
    clip_epsilon: float = Field(default=0.2, description="PPOè£å‰ªå‚æ•°")
    ppo_epochs: int = Field(default=4, description="PPOè®­ç»ƒè½®æ•°")


@dataclass
class AgentStats:
    """Agentç»Ÿè®¡ä¿¡æ¯"""
    agent_type: str
    buffer_size: int = 0
    epsilon: float = 0.0
    avg_loss: float = 0.0
    train_steps: int = 0
    current_buffer: int = 0
    ppo_epochs: int = 0
    avg_policy_loss: float = 0.0
    avg_value_loss: float = 0.0
    total_reuses: int = 0
    data_usage: str = ""


@dataclass
class TrainingResult:
    """è®­ç»ƒç»“æœ"""
    episode_rewards: List[float]
    episode_steps: List[int]
    total_time: float
    best_reward: float
    final_avg_reward: float


@dataclass
class ComparisonResult:
    """å¯¹æ¯”ç»“æœ"""
    algorithm_name: str
    avg_last_50: float
    max_reward: float
    success_rate: float


# ==================== è¿·å®«ç¯å¢ƒ ====================

class MazeEnv:
    """
    è¿·å®«ç¯å¢ƒå®šä¹‰ï¼š
    R = Road (å¯é€šè¡Œï¼Œå¥–åŠ±0)
    T = Trap (é™·é˜±ï¼Œå¥–åŠ±-10)
    W = Wall (å¢™å£ï¼Œä¸å¯é€šè¡Œ)
    G = Goal (ç›®æ ‡ï¼Œå¥–åŠ±+100ï¼Œç»ˆæ­¢)
    B = Bonus (å¥–åŠ±ç‚¹ï¼Œå¥–åŠ±+10)
    """
    
    def __init__(self, maze_map: List[str]) -> None:
        self.maze_map: List[List[str]] = [list(row) for row in maze_map]
        self.height: int = len(self.maze_map)
        self.width: int = len(self.maze_map[0])
        
        # æ‰¾åˆ°èµ·å§‹ä½ç½®ï¼ˆç¬¬ä¸€ä¸ªRæˆ–ç¬¬ä¸€ä¸ªéWä½ç½®ï¼‰
        self.start_pos: List[int] = self._find_start()
        self.agent_pos: List[int] = list(self.start_pos)
        
        # åŠ¨ä½œç©ºé—´ï¼šä¸Šã€ä¸‹ã€å·¦ã€å³
        self.action_space: int = 4
        # çŠ¶æ€ç©ºé—´ï¼šä½ç½®ç¼–ç 
        self.state_space: int = self.height * self.width
        
        self.step_count: int = 0
        self.max_steps: int = 200
        
    def _find_start(self) -> List[int]:
        """æ‰¾åˆ°èµ·å§‹ä½ç½®"""
        for i in range(self.height):
            for j in range(self.width):
                if self.maze_map[i][j] == 'R':
                    return [i, j]
        return [0, 0]
    
    def reset(self) -> int:
        """é‡ç½®ç¯å¢ƒ"""
        self.agent_pos = list(self.start_pos)
        self.step_count = 0
        return self._get_state()
    
    def _get_state(self) -> int:
        """è·å–å½“å‰çŠ¶æ€ï¼ˆä½ç½®ç¼–ç ï¼‰"""
        return self.agent_pos[0] * self.width + self.agent_pos[1]
    
    def step(self, action: int) -> StepResult:
        """æ‰§è¡ŒåŠ¨ä½œ"""
        self.step_count += 1
        
        # åŠ¨ä½œæ˜ å°„ï¼š0=ä¸Š, 1=ä¸‹, 2=å·¦, 3=å³
        moves = [
            ACTION_DELTAS[ord] 
            for ord in ACTION_ORDER
        ]
        next_pos: List[int] = [
            self.agent_pos[0] + moves[action][0],
            self.agent_pos[1] + moves[action][1]
        ]
        
        # æ£€æŸ¥è¾¹ç•Œ
        if not (0 <= next_pos[0] < self.height and 0 <= next_pos[1] < self.width):
            info = StepInfo(hit='boundary')
            return StepResult(
                state=self._get_state(),
                reward=-5.0,
                done=False,
                info=info
            )
        
        # æ£€æŸ¥å¢™å£
        cell: str = self.maze_map[next_pos[0]][next_pos[1]]
        if cell == 'W':
            info = StepInfo(hit='wall')
            return StepResult(
                state=self._get_state(),
                reward=-2.0,
                done=False,
                info=info
            )
        
        # ç§»åŠ¨åˆ°æ–°ä½ç½®
        self.agent_pos = next_pos
        
        # è®¡ç®—å¥–åŠ±
        reward: float = 0.0
        done: bool = False
        info = StepInfo()
        
        if cell == 'R':
            reward = -1  # å°æƒ©ç½šé¼“åŠ±å¿«é€Ÿåˆ°è¾¾
            info.hit = 'road'
        elif cell == 'T':
            reward = -10.0
            info.hit = 'trap'
        elif cell == 'B':
            reward = 15.0
            info.hit = 'bonus'
            self.maze_map[next_pos[0]][next_pos[1]] = 'R'  # å¥–åŠ±åªèƒ½æ‹¿ä¸€æ¬¡
        elif cell == 'G':
            reward = 100.0
            done = True
            info.hit = 'goal'
        
        # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
        if self.step_count >= self.max_steps:
            done = True
            info.timeout = True
        
        return StepResult(
            state=self._get_state(),
            reward=reward,
            done=done,
            info=info
        )
    
    def render(self) -> None:
        """æ¸²æŸ“è¿·å®«"""
        print("\n" + "="*40)
        for i in range(self.height):
            row: str = ""
            for j in range(self.width):
                if [i, j] == self.agent_pos:
                    row += "ğŸ¤– "
                else:
                    cell: str = self.maze_map[i][j]
                    symbols: List[Tuple[str, str]] = [
                        ('R', 'â¬œ'), ('T', 'ğŸ’¥'), ('W', 'â¬›'),
                        ('G', 'ğŸ¯'), ('B', 'ğŸ’')
                    ]
                    symbol: str = cell
                    for c, s in symbols:
                        if cell == c:
                            symbol = s
                            break
                    row += symbol + " "
            print(row)
        print("="*40)


# ==================== ç¥ç»ç½‘ç»œ ====================

class QNetwork(nn.Module):
    """DQNçš„Qç½‘ç»œ"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128) -> None:
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.net(x)


class PolicyNetwork(nn.Module):
    """ç­–ç•¥ç½‘ç»œ"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128) -> None:
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return F.softmax(self.net(x), dim=-1)


class ValueNetwork(nn.Module):
    """ä»·å€¼ç½‘ç»œï¼ˆç”¨äºActor-Criticï¼‰"""
    
    def __init__(self, state_dim: int, hidden_dim: int = 128) -> None:
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.net(x)


# ==================== åŸºç¡€Agentæ¥å£ ====================

class BaseAgent:
    """å¯æ’æ‹”AgentåŸºç±»"""
    
    def __init__(self, state_dim: int, action_dim: int) -> None:
        self.state_dim: int = state_dim
        self.action_dim: int = action_dim
        self.episode_count: int = 0
        
    def select_action(self, state: int, training: bool = True) -> int:
        """é€‰æ‹©åŠ¨ä½œ"""
        raise NotImplementedError
    
    def store_transition(
        self, 
        state: int, 
        action: int, 
        reward: float, 
        next_state: int, 
        done: bool
    ) -> None:
        """å­˜å‚¨è½¬æ¢"""
        raise NotImplementedError
    
    def train(self) -> Optional[float]:
        """è®­ç»ƒ"""
        raise NotImplementedError
    
    def get_stats(self) -> AgentStats:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        raise NotImplementedError


# ==================== DQN Agent (Off-Policy) ====================

class DQNAgent(BaseAgent):
    """DQN Agent"""
    
    def __init__(
        self, 
        state_dim: int, 
        action_dim: int, 
        config: Optional[DQNConfig] = None
    ) -> None:
        super().__init__(state_dim, action_dim)
        
        self.config: DQNConfig = config if config else DQNConfig()
        
        self.q_net: QNetwork = QNetwork(
            state_dim, 
            action_dim, 
            self.config.hidden_dim
        )
        self.target_net: QNetwork = QNetwork(
            state_dim, 
            action_dim, 
            self.config.hidden_dim
        )
        self.target_net.load_state_dict(self.q_net.state_dict())
        
        self.optimizer: torch.optim.Adam = torch.optim.Adam(
            self.q_net.parameters(), 
            lr=self.config.learning_rate
        )
        
        self.replay_buffer: Deque[Transition] = deque(maxlen=self.config.buffer_size)
        self.epsilon: float = self.config.epsilon
        
        self.train_steps: int = 0
        self.total_loss: float = 0.0
        self.loss_count: int = 0
        
    def select_action(self, state: int, training: bool = True) -> int:
        if training and random.random() < self.epsilon:
            return random.randint(0, self.action_dim - 1)
        
        state_tensor: torch.Tensor = F.one_hot(
            torch.tensor(state), 
            self.state_dim
        ).float()
        with torch.no_grad():
            q_values: torch.Tensor = self.q_net(state_tensor)
        return int(q_values.argmax().item())
    
    def store_transition(
        self, 
        state: int, 
        action: int, 
        reward: float, 
        next_state: int, 
        done: bool
    ) -> None:
        transition = Transition(
            state=state,
            action=action,
            reward=reward,
            next_state=next_state,
            done=done
        )
        self.replay_buffer.append(transition)
    
    def train(self) -> Optional[float]:
        if len(self.replay_buffer) < self.config.batch_size:
            return None
        
        # ä»ç»éªŒå›æ”¾ä¸­é‡‡æ ·
        batch: List[Transition] = random.sample(
            self.replay_buffer, 
            self.config.batch_size
        )
        
        states: List[int] = [t.state for t in batch]
        actions: List[int] = [t.action for t in batch]
        rewards: List[float] = [t.reward for t in batch]
        next_states: List[int] = [t.next_state for t in batch]
        dones: List[bool] = [t.done for t in batch]
        
        # è½¬æ¢ä¸ºtensor
        states_tensor: torch.Tensor = F.one_hot(
            torch.tensor(states), 
            self.state_dim
        ).float()
        actions_tensor: torch.Tensor = torch.tensor(actions).long()
        rewards_tensor: torch.Tensor = torch.tensor(rewards).float()
        next_states_tensor: torch.Tensor = F.one_hot(
            torch.tensor(next_states), 
            self.state_dim
        ).float()
        dones_tensor: torch.Tensor = torch.tensor(dones).float()
        
        # è®¡ç®—å½“å‰Qå€¼
        current_q: torch.Tensor = self.q_net(states_tensor).gather(
            1, 
            actions_tensor.unsqueeze(1)
        ).squeeze()
        
        # è®¡ç®—ç›®æ ‡Qå€¼
        with torch.no_grad():
            next_q: torch.Tensor = self.target_net(next_states_tensor).max(1)[0]
            target_q: torch.Tensor = rewards_tensor + \
                self.config.gamma * next_q * (1 - dones_tensor)
        
        # è®¡ç®—æŸå¤±
        loss: torch.Tensor = F.mse_loss(current_q, target_q)
        
        # ä¼˜åŒ–
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        # æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self.train_steps += 1
        if self.train_steps % self.config.update_target_freq == 0:
            self.target_net.load_state_dict(self.q_net.state_dict())
        
        # è¡°å‡epsilon
        self.epsilon = max(
            self.config.epsilon_min, 
            self.epsilon * self.config.epsilon_decay
        )
        
        loss_value: float = float(loss.item())
        self.total_loss += loss_value
        self.loss_count += 1
        
        return loss_value
    
    def get_stats(self) -> AgentStats:
        avg_loss: float = self.total_loss / max(1, self.loss_count)
        return AgentStats(
            agent_type='Off-Policy (DQN)',
            buffer_size=len(self.replay_buffer),
            epsilon=self.epsilon,
            avg_loss=avg_loss,
            train_steps=self.train_steps,
            data_usage='âˆ (experience replay)'
        )


# ==================== Policy Gradient Agent (On-Policy) ====================

class PGAgent(BaseAgent):
    """Policy Gradient Agent"""
    
    def __init__(
        self, 
        state_dim: int, 
        action_dim: int, 
        config: Optional[TrainingConfig] = None
    ) -> None:
        super().__init__(state_dim, action_dim)
        
        self.config: TrainingConfig = config if config else TrainingConfig()
        
        self.policy_net: PolicyNetwork = PolicyNetwork(
            state_dim, 
            action_dim, 
            self.config.hidden_dim
        )
        self.optimizer: torch.optim.Adam = torch.optim.Adam(
            self.policy_net.parameters(), 
            lr=self.config.learning_rate
        )
        
        self.episode_data: List[Tuple[int, int, float]] = []
        self.total_loss: float = 0.0
        self.loss_count: int = 0
        
    def select_action(self, state: int, training: bool = True) -> int:
        state_tensor: torch.Tensor = F.one_hot(
            torch.tensor(state), 
            self.state_dim
        ).float()
        probs: torch.Tensor = self.policy_net(state_tensor)
        
        if training:
            dist = torch.distributions.Categorical(probs)
            action: torch.Tensor = dist.sample()
            return int(action.item())
        else:
            return int(probs.argmax().item())
    
    def store_transition(
        self, 
        state: int, 
        action: int, 
        reward: float, 
        next_state: int, 
        done: bool
    ) -> None:
        self.episode_data.append((state, action, reward))
    
    def train(self) -> Optional[float]:
        if len(self.episode_data) == 0:
            return None
        
        states: List[int] = [t[0] for t in self.episode_data]
        actions: List[int] = [t[1] for t in self.episode_data]
        rewards: List[float] = [t[2] for t in self.episode_data]
        
        # è®¡ç®—å›æŠ¥
        returns: List[float] = []
        G: float = 0.0
        for r in reversed(rewards):
            G = r + self.config.gamma * G
            returns.insert(0, G)
        
        # æ ‡å‡†åŒ–å›æŠ¥
        returns_tensor: torch.Tensor = torch.tensor(returns).float()
        returns_tensor = (returns_tensor - returns_tensor.mean()) / \
            (returns_tensor.std() + 1e-8)
        
        # è®¡ç®—ç­–ç•¥æ¢¯åº¦
        states_tensor: torch.Tensor = F.one_hot(
            torch.tensor(states), 
            self.state_dim
        ).float()
        actions_tensor: torch.Tensor = torch.tensor(actions).long()
        
        probs: torch.Tensor = self.policy_net(states_tensor)
        log_probs: torch.Tensor = torch.log(
            probs.gather(1, actions_tensor.unsqueeze(1)).squeeze() + 1e-8
        )
        
        loss: torch.Tensor = -(log_probs * returns_tensor).mean()
        
        # ä¼˜åŒ–
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        loss_value: float = float(loss.item())
        self.total_loss += loss_value
        self.loss_count += 1
        
        # âœ… On-Policy: ç”¨å®Œå³ä¸¢
        self.episode_data.clear()
        
        return loss_value
    
    def get_stats(self) -> AgentStats:
        avg_loss: float = self.total_loss / max(1, self.loss_count)
        return AgentStats(
            agent_type='On-Policy (PG)',
            current_buffer=len(self.episode_data),
            avg_loss=avg_loss,
            data_usage='1x (immediate discard)'
        )


# ==================== PPO Agent (On-Policy with Limited Reuse) ====================

class PPOAgent(BaseAgent):
    """PPO Agent"""
    
    def __init__(
        self, 
        state_dim: int, 
        action_dim: int, 
        config: Optional[PPOConfig] = None
    ) -> None:
        super().__init__(state_dim, action_dim)
        
        self.config: PPOConfig = config if config else PPOConfig()
        
        self.policy_net: PolicyNetwork = PolicyNetwork(
            state_dim, 
            action_dim, 
            self.config.hidden_dim
        )
        self.value_net: ValueNetwork = ValueNetwork(
            state_dim, 
            self.config.hidden_dim
        )
        
        self.policy_optimizer: torch.optim.Adam = torch.optim.Adam(
            self.policy_net.parameters(), 
            lr=self.config.learning_rate
        )
        self.value_optimizer: torch.optim.Adam = torch.optim.Adam(
            self.value_net.parameters(), 
            lr=self.config.learning_rate
        )
        
        self.episode_data: List[PPOTransition] = []
        self.total_policy_loss: float = 0.0
        self.total_value_loss: float = 0.0
        self.loss_count: int = 0
        self.reuse_count: int = 0
        
    def select_action(self, state: int, training: bool = True) -> int:
        state_tensor: torch.Tensor = F.one_hot(
            torch.tensor(state), 
            self.state_dim
        ).float()
        probs: torch.Tensor = self.policy_net(state_tensor)
        
        if training:
            dist = torch.distributions.Categorical(probs)
            action: torch.Tensor = dist.sample()
            return int(action.item())
        else:
            return int(probs.argmax().item())
    
    def store_transition(
        self, 
        state: int, 
        action: int, 
        reward: float, 
        next_state: int, 
        done: bool
    ) -> None:
        state_tensor: torch.Tensor = F.one_hot(
            torch.tensor(state), 
            self.state_dim
        ).float()
        
        with torch.no_grad():
            probs: torch.Tensor = self.policy_net(state_tensor)
            old_log_prob: torch.Tensor = torch.log(probs[action] + 1e-8)
            value: torch.Tensor = self.value_net(state_tensor)
        
        transition = PPOTransition(
            state=state,
            action=action,
            reward=reward,
            old_log_prob=float(old_log_prob.item()),
            value=float(value.item())
        )
        self.episode_data.append(transition)
    
    def train(self) -> Optional[float]:
        if len(self.episode_data) == 0:
            return None
        
        states: List[int] = [t.state for t in self.episode_data]
        actions: List[int] = [t.action for t in self.episode_data]
        rewards: List[float] = [t.reward for t in self.episode_data]
        old_log_probs: List[float] = [t.old_log_prob for t in self.episode_data]
        values: List[float] = [t.value for t in self.episode_data]
        
        # è®¡ç®—ä¼˜åŠ¿å‡½æ•°
        returns: List[float] = []
        advantages: List[float] = []
        G: float = 0.0
        A: float = 0.0
        
        for i in reversed(range(len(rewards))):
            G = rewards[i] + self.config.gamma * G
            delta: float = rewards[i] + self.config.gamma * \
                (values[i+1] if i+1 < len(values) else 0.0) - values[i]
            A = delta + self.config.gamma * self.config.gae_lambda * A
            
            returns.insert(0, G)
            advantages.insert(0, A)
        
        # è½¬æ¢ä¸ºtensor
        states_tensor: torch.Tensor = F.one_hot(
            torch.tensor(states), 
            self.state_dim
        ).float()
        actions_tensor: torch.Tensor = torch.tensor(actions).long()
        returns_tensor: torch.Tensor = torch.tensor(returns).float()
        advantages_tensor: torch.Tensor = torch.tensor(advantages).float()
        advantages_tensor = (advantages_tensor - advantages_tensor.mean()) / \
            (advantages_tensor.std() + 1e-8)
        old_log_probs_tensor: torch.Tensor = torch.tensor(old_log_probs).float()
        
        # âš ï¸ PPO: å¤šè½®è®­ç»ƒï¼ˆæœ‰é™é‡ç”¨æ•°æ®ï¼‰
        total_policy_loss: float = 0.0
        total_value_loss: float = 0.0
        
        for epoch in range(self.config.ppo_epochs):
            # è®¡ç®—å½“å‰ç­–ç•¥çš„log_prob
            probs: torch.Tensor = self.policy_net(states_tensor)
            log_probs: torch.Tensor = torch.log(
                probs.gather(1, actions_tensor.unsqueeze(1)).squeeze() + 1e-8
            )
            
            # è®¡ç®—é‡è¦æ€§é‡‡æ ·æ¯”ç‡
            ratio: torch.Tensor = torch.exp(log_probs - old_log_probs_tensor)
            
            # PPOè£å‰ª
            surr1: torch.Tensor = ratio * advantages_tensor
            surr2: torch.Tensor = torch.clamp(
                ratio, 
                1 - self.config.clip_epsilon, 
                1 + self.config.clip_epsilon
            ) * advantages_tensor
            policy_loss: torch.Tensor = -torch.min(surr1, surr2).mean()
            
            # ä»·å€¼å‡½æ•°æŸå¤±
            values_pred: torch.Tensor = self.value_net(states_tensor).squeeze()
            value_loss: torch.Tensor = F.mse_loss(values_pred, returns_tensor)
            
            # ä¼˜åŒ–ç­–ç•¥ç½‘ç»œ
            self.policy_optimizer.zero_grad()
            policy_loss.backward()
            self.policy_optimizer.step()
            
            # ä¼˜åŒ–ä»·å€¼ç½‘ç»œ
            self.value_optimizer.zero_grad()
            value_loss.backward()
            self.value_optimizer.step()
            
            total_policy_loss += float(policy_loss.item())
            total_value_loss += float(value_loss.item())
            self.reuse_count += 1
        
        avg_policy_loss: float = total_policy_loss / self.config.ppo_epochs
        avg_value_loss: float = total_value_loss / self.config.ppo_epochs
        
        self.total_policy_loss += avg_policy_loss
        self.total_value_loss += avg_value_loss
        self.loss_count += 1
        
        # âš ï¸ PPO: é‡ç”¨åä»éœ€æ¸…ç©º
        self.episode_data.clear()
        
        return avg_policy_loss
    
    def get_stats(self) -> AgentStats:
        avg_policy_loss: float = self.total_policy_loss / max(1, self.loss_count)
        avg_value_loss: float = self.total_value_loss / max(1, self.loss_count)
        return AgentStats(
            agent_type='On-Policy (PPO)',
            current_buffer=len(self.episode_data),
            ppo_epochs=self.config.ppo_epochs,
            avg_policy_loss=avg_policy_loss,
            avg_value_loss=avg_value_loss,
            total_reuses=self.reuse_count,
            data_usage=f'{self.config.ppo_epochs}x (limited reuse)'
        )


# ==================== è®­ç»ƒæ¡†æ¶ ====================

class MazeTrainer:
    """å¯æ’æ‹”çš„è®­ç»ƒæ¡†æ¶"""
    
    def __init__(self, env: MazeEnv, agent: BaseAgent) -> None:
        self.env: MazeEnv = env
        self.agent: BaseAgent = agent
        self.episode_rewards: List[float] = []
        self.episode_steps: List[int] = []
        
    def train(
        self, 
        num_episodes: int = 500, 
        print_freq: int = 50, 
        render_freq: int = 100
    ) -> TrainingResult:
        print(f"\n{'='*60}")
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒ: {self.agent.get_stats().agent_type}")
        print(f"{'='*60}\n")
        
        start_time: float = time.time()
        
        for episode in range(num_episodes):
            state: int = self.env.reset()
            episode_reward: float = 0.0
            episode_steps: int = 0
            done: bool = False
            
            # æ”¶é›†ä¸€ä¸ªepisodeçš„æ•°æ®
            while not done:
                action: int = self.agent.select_action(state, training=True)
                step_result: StepResult = self.env.step(action)
                
                self.agent.store_transition(
                    state, 
                    action, 
                    step_result.reward, 
                    step_result.state, 
                    step_result.done
                )
                
                state = step_result.state
                episode_reward += step_result.reward
                episode_steps += 1
                done = step_result.done
            
            # è®­ç»ƒ
            loss: Optional[float] = self.agent.train()
            
            self.episode_rewards.append(episode_reward)
            self.episode_steps.append(episode_steps)
            
            # æ‰“å°è®­ç»ƒä¿¡æ¯
            if (episode + 1) % print_freq == 0:
                avg_reward: float = float(
                    np.mean(self.episode_rewards[-print_freq:])
                )
                avg_steps: float = float(
                    np.mean(self.episode_steps[-print_freq:])
                )
                elapsed_time: float = time.time() - start_time
                
                print(f"\n{'â”€'*60}")
                print(f"ğŸ“Š Episode {episode + 1}/{num_episodes}")
                print(f"{'â”€'*60}")
                print(f"â±ï¸  Time: {elapsed_time:.2f}s")
                print(f"ğŸ¯ Avg Reward (last {print_freq}): {avg_reward:.2f}")
                print(f"ğŸ‘£ Avg Steps: {avg_steps:.2f}")
                print(f"ğŸ“‰ Loss: {loss:.4f}" if loss else "ğŸ“‰ Loss: warming up...")
                
                stats: AgentStats = self.agent.get_stats()
                print(f"\nğŸ“ˆ Agent Stats:")
                self._print_stats(stats)
                print(f"{'â”€'*60}")
            
            # æ¸²æŸ“
            if (episode + 1) % render_freq == 0:
                print(f"\nğŸ® Episode {episode + 1} æ¼”ç¤º:")
                self.demo(render=True)
        
        total_time: float = time.time() - start_time
        best_reward: float = float(max(self.episode_rewards))
        final_avg: float = float(np.mean(self.episode_rewards[-50:]))
        
        print(f"\n{'='*60}")
        print(f"âœ… è®­ç»ƒå®Œæˆ!")
        print(f"â±ï¸  Total Time: {total_time:.2f}s")
        print(f"ğŸ† Best Reward: {best_reward:.2f}")
        print(f"ğŸ“Š Final Avg Reward (last 50): {final_avg:.2f}")
        print(f"{'='*60}\n")
        
        return TrainingResult(
            episode_rewards=self.episode_rewards,
            episode_steps=self.episode_steps,
            total_time=total_time,
            best_reward=best_reward,
            final_avg_reward=final_avg
        )
    
    def _print_stats(self, stats: AgentStats) -> None:
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        if stats.buffer_size > 0:
            print(f"   buffer_size: {stats.buffer_size}")
        if stats.epsilon > 0:
            print(f"   epsilon: {stats.epsilon:.3f}")
        if stats.avg_loss > 0:
            print(f"   avg_loss: {stats.avg_loss:.4f}")
        if stats.train_steps > 0:
            print(f"   train_steps: {stats.train_steps}")
        if stats.current_buffer > 0:
            print(f"   current_buffer: {stats.current_buffer}")
        if stats.ppo_epochs > 0:
            print(f"   ppo_epochs: {stats.ppo_epochs}")
        if stats.avg_policy_loss > 0:
            print(f"   avg_policy_loss: {stats.avg_policy_loss:.4f}")
        if stats.avg_value_loss > 0:
            print(f"   avg_value_loss: {stats.avg_value_loss:.4f}")
        if stats.total_reuses > 0:
            print(f"   total_reuses: {stats.total_reuses}")
        if stats.data_usage:
            print(f"   data_usage: {stats.data_usage}")
    
    def demo(self, render: bool = True) -> float:
        """æ¼”ç¤ºè®­ç»ƒå¥½çš„agent"""
        state: int = self.env.reset()
        done: bool = False
        total_reward: float = 0.0
        steps: int = 0
        
        if render:
            self.env.render()
        
        while not done and steps < 50:
            action: int = self.agent.select_action(state, training=False)
            step_result: StepResult = self.env.step(action)
            state = step_result.state
            total_reward += step_result.reward
            steps += 1
            done = step_result.done
            
            if render:
                time.sleep(0.1)
                self.env.render()
                print(
                    f"Action: {ACTION_SYMBOLS[ACTION_ORDER[action]]}, "
                    f"Reward: {step_result.reward:.1f}, "
                    f"Hit: {step_result.info.hit}"
                )
        
        print(f"\nğŸ¯ Demo Result: Reward={total_reward:.2f}, Steps={steps}")
        return total_reward


# ==================== ä¸»ç¨‹åº ====================

def main() -> None:
    # å®šä¹‰è¿·å®«
    maze_design: List[str] = [
        "RWWWWWWWW",
        "RRRRTRRWG",
        "WBWWWRRWW",
        "WRRRWRRTR",
        "WRWRWWWRR",
        "WRRRRRWRB",
        "WRRWWRRRR",
    ]
    
    print("\n" + "="*60)
    print("ğŸ® è¿·å®«èµ°ä½ - On-Policy vs Off-Policy å¯¹æ¯”å®éªŒ")
    print("="*60)
    print("\nè¿·å®«å›¾ä¾‹:")
    print("  â¬œ R = Road (å¯é€šè¡Œ)")
    print("  ğŸ’¥ T = Trap (é™·é˜±, -10)")
    print("  â¬› W = Wall (å¢™å£)")
    print("  ğŸ¯ G = Goal (ç›®æ ‡, +100)")
    print("  ğŸ’ B = Bonus (å¥–åŠ±, +10)")
    print("  ğŸ¤– = Agent")
    
    # åˆ›å»ºç¯å¢ƒ
    env: MazeEnv = MazeEnv(maze_design)
    env.render()
    
    # è®­ç»ƒå‚æ•°
    num_episodes: int = 300
    
    # ==================== å®éªŒ1: DQN (Off-Policy) ====================
    print("\n\n" + "ğŸ”µ"*30)
    print("å®éªŒ 1: DQN (Off-Policy)")
    print("ğŸ”µ"*30)
    
    env1: MazeEnv = MazeEnv(maze_design)
    dqn_config: DQNConfig = DQNConfig(learning_rate=0.001)
    agent1: DQNAgent = DQNAgent(env1.state_space, env1.action_space, dqn_config)
    trainer1: MazeTrainer = MazeTrainer(env1, agent1)
    result1: TrainingResult = trainer1.train(
        num_episodes=num_episodes, 
        print_freq=50, 
        render_freq=150
    )

    import pdb; pdb.set_trace()
    
    # ==================== å®éªŒ2: Policy Gradient (On-Policy) ====================
    print("\n\n" + "ğŸŸ¢"*30)
    print("å®éªŒ 2: Policy Gradient (On-Policy)")
    print("ğŸŸ¢"*30)
    
    env2: MazeEnv = MazeEnv(maze_design)
    pg_config: TrainingConfig = TrainingConfig(learning_rate=0.001)
    agent2: PGAgent = PGAgent(env2.state_space, env2.action_space, pg_config)
    trainer2: MazeTrainer = MazeTrainer(env2, agent2)
    result2: TrainingResult = trainer2.train(
        num_episodes=num_episodes, 
        print_freq=50, 
        render_freq=150
    )
    
    # ==================== å®éªŒ3: PPO (On-Policy with Limited Reuse) ====================
    print("\n\n" + "ğŸŸ¡"*30)
    print("å®éªŒ 3: PPO (On-Policy with Limited Reuse)")
    print("ğŸŸ¡"*30)
    
    env3: MazeEnv = MazeEnv(maze_design)
    ppo_config: PPOConfig = PPOConfig(learning_rate=0.0003)
    agent3: PPOAgent = PPOAgent(env3.state_space, env3.action_space, ppo_config)
    trainer3: MazeTrainer = MazeTrainer(env3, agent3)
    result3: TrainingResult = trainer3.train(
        num_episodes=num_episodes, 
        print_freq=50, 
        render_freq=150
    )
    
    # ==================== æœ€ç»ˆå¯¹æ¯” ====================
    print("\n\n" + "="*80)
    print("ğŸ æœ€ç»ˆå¯¹æ¯”")
    print("="*80)
    
    results: List[Tuple[str, TrainingResult]] = [
        ("DQN (Off-Policy)", result1),
        ("PG (On-Policy)", result2),
        ("PPO (On-Policy)", result3)
    ]
    
    comparison_results: List[ComparisonResult] = []
    
    for name, result in results:
        avg_reward: float = float(np.mean(result.episode_rewards[-50:]))
        max_reward: float = float(max(result.episode_rewards))
        success_rate: float = float(
            sum(1 for r in result.episode_rewards[-50:] if r > 50) / 50 * 100
        )
        
        comparison_results.append(ComparisonResult(
            algorithm_name=name,
            avg_last_50=avg_reward,
            max_reward=max_reward,
            success_rate=success_rate
        ))
    
    print(f"\n{'Algorithm':<25} {'Avg Last 50':<15} {'Max Reward':<15} {'Success Rate':<15}")
    print("-"*80)
    
    for comp_result in comparison_results:
        print(
            f"{comp_result.algorithm_name:<25} "
            f"{comp_result.avg_last_50:>10.2f}     "
            f"{comp_result.max_reward:>10.2f}     "
            f"{comp_result.success_rate:>10.1f}%"
        )
    
    print("="*80)
    
    print("\n\n" + "ğŸ“š"*30)
    print("å…³é”®å·®å¼‚æ€»ç»“")
    print("ğŸ“š"*30)
    
    stats1: AgentStats = agent1.get_stats()
    stats2: AgentStats = agent2.get_stats()
    stats3: AgentStats = agent3.get_stats()
    
    print("\nğŸ”µ DQN (Off-Policy):")
    print(f"   âœ… ä½¿ç”¨ç»éªŒå›æ”¾ï¼Œæ•°æ®æ•ˆç‡é«˜")
    print(f"   âœ… å¯ä»¥é‡å¤ä½¿ç”¨å†å²æ•°æ®æ•°åƒæ¬¡")
    print(f"   âœ… Buffer Size: {stats1.buffer_size}")
    print(f"   âœ… Total Train Steps: {stats1.train_steps}")
    
    print("\nğŸŸ¢ Policy Gradient (On-Policy):")
    print(f"   âš ï¸  åªä½¿ç”¨å½“å‰ç­–ç•¥çš„æ–°æ•°æ®")
    print(f"   âš ï¸  æ•°æ®ç”¨å®Œå³ä¸¢ï¼Œæ•°æ®æ•ˆç‡ä½")
    print(f"   âš ï¸  {stats2.data_usage}")
    
    print("\nğŸŸ¡ PPO (On-Policy with Limited Reuse):")
    print(f"   â­ é€šè¿‡é‡è¦æ€§é‡‡æ ·å®ç°æœ‰é™é‡ç”¨")
    print(f"   â­ æ¯æ‰¹æ•°æ®é‡ç”¨ {stats3.ppo_epochs} æ¬¡")
    print(f"   â­ æ€»é‡ç”¨æ¬¡æ•°: {stats3.total_reuses}")
    print(f"   â­ {stats3.data_usage}")
    
    print("\n" + "="*80 + "\n")


if __name__ == "__main__":
    main()
